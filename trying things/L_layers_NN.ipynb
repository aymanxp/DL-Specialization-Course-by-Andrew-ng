{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee478207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5a3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[int(l)], layer_dims[int(l-1)]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[int(l)], 1))\n",
    "        \n",
    "\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08ed1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172],\n",
       "        [-0.01072969,  0.00865408, -0.02301539]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.01744812, -0.00761207]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters([3, 2, 1])\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21325513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z)) \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def leaky_relu(): \n",
    "    A = np.max(0.1 * Z, Z) \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def backward_sigmoid(dA, cache):\n",
    "    Z = cache \n",
    "    dZ = dA * (sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def backward_leaky_relu(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA)\n",
    "\n",
    "    return  dZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c80a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b): \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation): \n",
    "\n",
    "    if activation == \"leaky_relu\": \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a209898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, parameters): \n",
    "    caches = []\n",
    "    A = X \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev=A_prev,\n",
    "                                            W=parameters[\"W\" + str(l)],\n",
    "                                            b=parameters[\"b\" + str(l)], \n",
    "                                            activation=\"leaky_relu\")\n",
    "\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A_prev=A,\n",
    "                                            W=parameters[\"W\" + str(L)],\n",
    "                                            b=parameters[\"b\" + str(L)], \n",
    "                                            activation=\"sigmoid\")\n",
    "    cachesa.append(cache)\n",
    "    return AL, caches\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1d42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y): \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = - (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T)) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4c156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache): \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m \n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation): \n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"sigmoid\": \n",
    "        dZ = backward_sigmoid(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"leaky_relu\": \n",
    "        dZ = backward_leaky_relu(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad6b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(AL, Y, caches): \n",
    "    \n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches)\n",
    "\n",
    "    dAL = - np.divide(Y, AL) + np.divide(1 - Y, 1 - AL)\n",
    "    current_cache = caches[L- 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA = dAL, cache = current_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "\n",
    "    for l in reversed(range(L - 1)): # from l=L-2 to L=0 \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation=\"leaky_relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b3f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate): \n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        # parameters[\"W\" + str(l+1)] = ...\n",
    "        # parameters[\"b\" + str(l+1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        # YOUR CODE ENDS HERE\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02a13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(layers_dims, X, Y, num_iterations, learning_rate = 0.0075, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    \n",
    "    for i in range(num_iterations): \n",
    "        AL, caches = forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_cost and i % 100 == 0: \n",
    "            print(f\"iteration : {i}, cost: {cost}\")\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fbeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d1fa1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648a59d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526fad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "665e2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Enter 'c' at the ipdb>  prompt to continue execution.\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[31mNameError\u001b[39m: name 'magic' is not defined\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mdebug\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmagic\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m parameters, costs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmodel\u001b[39m\u001b[34m(layers_dims, X, Y, num_iterations, learning_rate, print_cost)\u001b[39m\n\u001b[32m      3\u001b[39m np.random.seed(\u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m costs = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m parameters = \u001b[43minitialize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations): \n\u001b[32m     10\u001b[39m     AL, caches = forward(X, parameters)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36minitialize_parameters\u001b[39m\u001b[34m(layer_dims)\u001b[39m\n\u001b[32m     14\u001b[39m L = \u001b[38;5;28mlen\u001b[39m(layer_dims)            \u001b[38;5;66;03m# number of layers in the network\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, L):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     parameters[\u001b[33m'\u001b[39m\u001b[33mW\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(l)] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m * \u001b[32m0.01\u001b[39m\n\u001b[32m     18\u001b[39m     parameters[\u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(l)] = np.zeros((layer_dims[\u001b[38;5;28mint\u001b[39m(l)], \u001b[32m1\u001b[39m))\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m parameters\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1306\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.randn\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1466\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.standard_normal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_common.pyx:655\u001b[39m, in \u001b[36mnumpy.random._common.cont\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "%debug magic\n",
    "parameters, costs = model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad212db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
